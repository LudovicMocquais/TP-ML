{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 : Multi-Layers Perceptron "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The objective of this lab is to dive into particular kind of neural network: the *Multi-Layer Perceptron* (MLP).\n",
    "\n",
    "To start, let us take the dataset from the previous lab (hydrodynamics of sailing boats) and use scikit-learn to train a MLP instead of our hand-made single perceptron.\n",
    "The code below is already complete and is meant to give you an idea of how to construct an MLP with scikit-learn. You can execute it, taking the time to understand the idea behind each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "import numpy as np\n",
    "dataset = np.genfromtxt(\"yacht_hydrodynamics.data\", delimiter='')\n",
    "X = dataset[:, :-1]\n",
    "Y = dataset[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: scale input data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y,random_state=1, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(max_iter=3000, random_state=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a multi-layer perceptron (MLP) network for regression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp = MLPRegressor(max_iter=3000, random_state=1) # define the model, with default params\n",
    "mlp.fit(x_train, y_train) # train the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.9940765369322633\n",
      "Test score:   0.9899773031580283\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print('Train score: ', mlp.score(x_train, y_train))\n",
    "print('Test score:  ', mlp.score(x_test, y_test))\n",
    "#plt.plot(mlp.loss_curve_)\n",
    "#plt.xlabel(\"Iterations\")\n",
    "#plt.ylabel(\"Loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "num_samples_to_plot = 20\n",
    "#plt.plot(y_test[0:num_samples_to_plot], 'ro', label='y')\n",
    "yw = mlp.predict(x_test)\n",
    "#plt.plot(yw[0:num_samples_to_plot], 'bx', label='$\\hat{y}$')\n",
    "#plt.legend()\n",
    "#plt.xlabel(\"Examples\")\n",
    "#plt.ylabel(\"f(examples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the network\n",
    "\n",
    "Many details of the network are currently hidden as default parameters.\n",
    "\n",
    "Using the [documentation of the MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html), answer the following questions.\n",
    "\n",
    "- What is the structure of the network?\n",
    " The network is a multi-layer perceptron with hidden layers which we can choose se number and the activation function.\n",
    "- What it is the algorithm used for training? Is there algorithm available that we mentioned during the courses?\n",
    "The training algorithm is the stochastic gradient descent. Yes it was mentionned during the courses.\n",
    "- How does the training algorithm decides to stop the training?\n",
    "We have tol that is the minimum desired improvement of the loss or score for consecutive iterations. The training stops when when there is no change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onto a more challenging dataset: house prices\n",
    "\n",
    "For the rest of this lab, we will use the (more challenging) [California Housing Prices dataset](https://www.kaggle.com/datasets/camnugent/california-housing-prices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean all previously defined variables for the sailing boats\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0368</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.761658</td>\n",
       "      <td>1.103627</td>\n",
       "      <td>413.0</td>\n",
       "      <td>2.139896</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.6591</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.931907</td>\n",
       "      <td>0.951362</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>2.128405</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.1200</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.797527</td>\n",
       "      <td>1.061824</td>\n",
       "      <td>1157.0</td>\n",
       "      <td>1.788253</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0804</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4.294118</td>\n",
       "      <td>1.117647</td>\n",
       "      <td>1206.0</td>\n",
       "      <td>2.026891</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.6912</td>\n",
       "      <td>52.0</td>\n",
       "      <td>4.970588</td>\n",
       "      <td>0.990196</td>\n",
       "      <td>1551.0</td>\n",
       "      <td>2.172269</td>\n",
       "      <td>37.84</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "5  4.0368      52.0  4.761658   1.103627       413.0  2.139896     37.85   \n",
       "6  3.6591      52.0  4.931907   0.951362      1094.0  2.128405     37.84   \n",
       "7  3.1200      52.0  4.797527   1.061824      1157.0  1.788253     37.84   \n",
       "8  2.0804      42.0  4.294118   1.117647      1206.0  2.026891     37.84   \n",
       "9  3.6912      52.0  4.970588   0.990196      1551.0  2.172269     37.84   \n",
       "\n",
       "   Longitude  \n",
       "0    -122.23  \n",
       "1    -122.22  \n",
       "2    -122.24  \n",
       "3    -122.25  \n",
       "4    -122.25  \n",
       "5    -122.25  \n",
       "6    -122.25  \n",
       "7    -122.25  \n",
       "8    -122.26  \n",
       "9    -122.25  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Import the required modules\"\"\"\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "\n",
    "num_samples = 2000 # only use the first N samples to limit training time\n",
    "\n",
    "cal_housing = fetch_california_housing()\n",
    "X = pd.DataFrame(cal_housing.data,columns=cal_housing.feature_names)[:num_samples]\n",
    "Y = cal_housing.target[:num_samples]\n",
    "X.head(10) # print the first 10 values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each row of the dataset represents a **group of houses** (one district). The `target` variable denotes the average house value in units of 100.000 USD. Median Income is per 10.000 USD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting a subpart of the dataset for testing\n",
    "\n",
    "- Split the dataset between a training set (75%) and a test set (25%)\n",
    "\n",
    "Please use the conventional names `X_train`, `X_test`, `y_train` and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y,random_state=1, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling the input data\n",
    "\n",
    "\n",
    "A step of **scaling** of the data is often useful to ensure that all input data centered on 0 and with a fixed variance.\n",
    "\n",
    "Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance). The function `StandardScaler` from `sklearn.preprocessing` computes the standard score of a sample as:\n",
    "\n",
    "```\n",
    "z = (x - u) / s\n",
    "```\n",
    "\n",
    "where `u` is the mean of the training samples, and `s` is the standard deviation of the training samples.\n",
    "\n",
    "Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using transform.\n",
    "\n",
    " - Apply the standard scaler to both the training dataset (`X_train`) and the test dataset (`X_test`).\n",
    " - Make sure that **exactly the same transformation** is applied to both datasets.\n",
    "\n",
    "[Documentation of standard scaler in scikit learn](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(x_train)\n",
    "X_test = sc.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "In this part, we are only interested in maximizing the **train score**, i.e., having the network memorize the training examples as well as possible.\n",
    "\n",
    "- Propose a parameterization of the network (shape and learning parameters) that will maximize the train score (without considering the test score).\n",
    "   \n",
    "   While doing this, you should (1) remain within two minutes of training time, and (2) obtain a score that is greater than 0.90.\n",
    "   \n",
    "\n",
    "- Is the **test** score substantially smaller than the **train** score (indicator of overfitting) ?\n",
    "    \n",
    "    Yes because with a good train score we tend to overfit\n",
    "    \n",
    "    \n",
    "- Explain how the parameters you chose allow the learned model to overfit.\n",
    "   \n",
    "   To increase the train score, we can start by adding mode hidden layers to predicate better the training value of the houses and delete the validation fraction so that all of the set is used for training. We can also decrease the tolerance, increase the number of iterations of the algorithm to get a more precise results.\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score:  0.9143311098389366\n",
      "Test score:   0.3194622548310331\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(100,100,100), tol = 1e-8,\n",
    "                   learning_rate='adaptive',validation_fraction=0, \n",
    "                    max_iter = 2500)\n",
    "\n",
    "\n",
    "mlp.fit(X_train, y_train) # train the MLP\n",
    "\n",
    "print('Train score: ', mlp.score(X_train, y_train))\n",
    "print('Test score:  ', mlp.score(X_test, y_test))\n",
    "plt.plot(mlp.loss_curve_)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "In this section, we are now interested in maximizing the ability of the network to predict the value of unseen examples, i.e., maximizing the **test** score.\n",
    "You should experiment with the possible parameters of the network in order to obtain a good test score, ideally with a small learning time.\n",
    "\n",
    "Parameters to vary:\n",
    "\n",
    "- number and size of the hidden layers\n",
    "- activation function\n",
    "- stopping conditions\n",
    "- maximum number of iterations\n",
    "- initial learning rate value\n",
    "\n",
    "Results to present for the tested configurations:\n",
    "\n",
    "- Train/test score\n",
    "- training time\n",
    "\n",
    "\n",
    "Present in a table the various parameters tested and the associated results. You can find in the last cell of the notebook a code snippet that will allow you to plot tables from python structure.\n",
    "Be methodical in the way your run your experiments and collect data. For each run, you should record the parameters and results into an external data structure.\n",
    "\n",
    "(Note that, while we encourage you to explore the solution space manually, there are existing methods in scikit-learn and other learning framework to automate this step as well, e.g., [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/insa/anaconda/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "from time import time\n",
    "data = []\n",
    "\n",
    "def test_conf(conf) :\n",
    "    \n",
    "    t = time()\n",
    "    mlp = MLPRegressor(**conf)\n",
    "    mlp.fit(X_train, y_train) # train the MLP\n",
    "    conf[\"Time\"] = time()-t\n",
    "    \n",
    "    conf[\"Train score\"]=mlp.score(X_train, y_train)\n",
    "    conf[\"Test score\"]= mlp.score(X_test, y_test)\n",
    "    \n",
    "    data.append(conf)\n",
    "    return mlp\n",
    "    \n",
    "    \n",
    "conf_d = {\"hidden_layer_sizes\" : (100,), \"activation\" : 'relu', \"tol\" :0.0001, \"early_stopping\" : False, \"max_iter\":200, \"learning_rate_init\":0.001}\n",
    "test_conf(conf_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(100, 100, 100, 100))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf1 = {\"hidden_layer_sizes\" : (100,100,100, 100), \"activation\" : 'relu', \"tol\" :0.0001, \"early_stopping\" : False, \"max_iter\":200, \"learning_rate_init\":0.001}\n",
    "test_conf(conf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/insa/anaconda/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(hidden_layer_sizes=(10,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf2 = {\"hidden_layer_sizes\" : (10,), \"activation\" : 'relu', \"tol\" :0.0001, \"early_stopping\" : False, \"max_iter\":200, \"learning_rate_init\":0.001}\n",
    "test_conf(conf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='identity')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf3 = {\"hidden_layer_sizes\" : (100,), \"activation\" : 'identity', \"tol\" :0.0001, \"early_stopping\" : False, \"max_iter\":200, \"learning_rate_init\":0.001}\n",
    "test_conf(conf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/insa/anaconda/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='logistic')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "conf4 = {\"hidden_layer_sizes\" : (100,), \"activation\" : 'logistic', \"tol\" :0.0001, \"early_stopping\" : False, \"max_iter\":200, \"learning_rate_init\":0.001}\n",
    "test_conf(conf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/insa/anaconda/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='tanh')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "conf5 = {\"hidden_layer_sizes\" : (100,), \"activation\" : 'tanh', \"tol\" :0.0001, \"early_stopping\" : False, \"max_iter\":200, \"learning_rate_init\":0.001}\n",
    "test_conf(conf5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(tol=0.01)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf6 = {\"hidden_layer_sizes\" : (100,), \"activation\" : 'relu', \"tol\" :0.01, \"early_stopping\" : False, \"max_iter\":200, \"learning_rate_init\":0.001}\n",
    "test_conf(conf6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(early_stopping=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf7 = {\"hidden_layer_sizes\" : (100,), \"activation\" : 'relu', \"tol\" :0.0001, \"early_stopping\" : True, \"max_iter\":200, \"learning_rate_init\":0.001}\n",
    "test_conf(conf7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(max_iter=2000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf8 = {\"hidden_layer_sizes\" : (100,), \"activation\" : 'relu', \"tol\" :0.0001, \"early_stopping\" : False, \"max_iter\":2000, \"learning_rate_init\":0.001}\n",
    "test_conf(conf8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf9 = {\"hidden_layer_sizes\" : (100,), \"activation\" : 'relu', \"tol\" :0.0001, \"early_stopping\" : False, \"max_iter\":200, \"learning_rate_init\":0.1}\n",
    "final_mlp = test_conf(conf9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPRegressor(early_stopping=True, hidden_layer_sizes=(100, 25),\n",
       "             learning_rate='adaptive', max_iter=500)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The best configuration is one that gives us the best score for both training and testing set\n",
    "#We should then compromise on the number of layers and the maximum of iterations\n",
    "\n",
    "best_conf = {\"hidden_layer_sizes\" : (100,25), \"activation\" : 'relu', \"tol\" :0.0001, \"early_stopping\" : True, \n",
    "             \"learning_rate\" : 'adaptive',\"validation_fraction\" : 0.1, \"max_iter\":500, \"learning_rate_init\":0.001 }\n",
    "test_conf(best_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_layer_sizes</th>\n",
       "      <th>activation</th>\n",
       "      <th>tol</th>\n",
       "      <th>early_stopping</th>\n",
       "      <th>max_iter</th>\n",
       "      <th>learning_rate_init</th>\n",
       "      <th>Time</th>\n",
       "      <th>Train score</th>\n",
       "      <th>Test score</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>validation_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(100, 25)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>500</td>\n",
       "      <td>0.001</td>\n",
       "      <td>13.593365</td>\n",
       "      <td>0.800659</td>\n",
       "      <td>0.796612</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(100,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>True</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>18.896147</td>\n",
       "      <td>0.797888</td>\n",
       "      <td>0.785586</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(100,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>25.887275</td>\n",
       "      <td>0.818538</td>\n",
       "      <td>0.781142</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(100,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.804348</td>\n",
       "      <td>0.739251</td>\n",
       "      <td>0.767593</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(100,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>0.100</td>\n",
       "      <td>4.988372</td>\n",
       "      <td>0.822721</td>\n",
       "      <td>0.765393</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(100,)</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>35.992563</td>\n",
       "      <td>0.788781</td>\n",
       "      <td>0.753041</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(100,)</td>\n",
       "      <td>logistic</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>28.594653</td>\n",
       "      <td>0.740026</td>\n",
       "      <td>0.752026</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(100,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>48.092507</td>\n",
       "      <td>0.838026</td>\n",
       "      <td>0.737605</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(100,)</td>\n",
       "      <td>identity</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.758420</td>\n",
       "      <td>0.688997</td>\n",
       "      <td>0.725396</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(10,)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.832452</td>\n",
       "      <td>0.736469</td>\n",
       "      <td>0.687174</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(100, 100, 100, 100)</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>False</td>\n",
       "      <td>200</td>\n",
       "      <td>0.001</td>\n",
       "      <td>36.901365</td>\n",
       "      <td>0.902037</td>\n",
       "      <td>0.566535</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      hidden_layer_sizes activation     tol  early_stopping  max_iter  \\\n",
       "10             (100, 25)       relu  0.0001            True       500   \n",
       "7                 (100,)       relu  0.0001            True       200   \n",
       "0                 (100,)       relu  0.0001           False       200   \n",
       "6                 (100,)       relu  0.0100           False       200   \n",
       "9                 (100,)       relu  0.0001           False       200   \n",
       "5                 (100,)       tanh  0.0001           False       200   \n",
       "4                 (100,)   logistic  0.0001           False       200   \n",
       "8                 (100,)       relu  0.0001           False      2000   \n",
       "3                 (100,)   identity  0.0001           False       200   \n",
       "2                  (10,)       relu  0.0001           False       200   \n",
       "1   (100, 100, 100, 100)       relu  0.0001           False       200   \n",
       "\n",
       "    learning_rate_init       Time  Train score  Test score learning_rate  \\\n",
       "10               0.001  13.593365     0.800659    0.796612      adaptive   \n",
       "7                0.001  18.896147     0.797888    0.785586             -   \n",
       "0                0.001  25.887275     0.818538    0.781142             -   \n",
       "6                0.001   2.804348     0.739251    0.767593             -   \n",
       "9                0.100   4.988372     0.822721    0.765393             -   \n",
       "5                0.001  35.992563     0.788781    0.753041             -   \n",
       "4                0.001  28.594653     0.740026    0.752026             -   \n",
       "8                0.001  48.092507     0.838026    0.737605             -   \n",
       "3                0.001   4.758420     0.688997    0.725396             -   \n",
       "2                0.001   0.832452     0.736469    0.687174             -   \n",
       "1                0.001  36.901365     0.902037    0.566535             -   \n",
       "\n",
       "   validation_fraction  \n",
       "10                 0.1  \n",
       "7                    -  \n",
       "0                    -  \n",
       "6                    -  \n",
       "9                    -  \n",
       "5                    -  \n",
       "4                    -  \n",
       "8                    -  \n",
       "3                    -  \n",
       "2                    -  \n",
       "1                    -  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "table = pd.DataFrame.from_dict(data)\n",
    "table = table.replace(np.nan, '-')\n",
    "table = table.sort_values(by='Test score', ascending=False)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "- From your experiments, what seems to be the best model (i.e. set of parameters) for predicting the value of a house?\n",
    "\n",
    "    - Influences of the parameters :\n",
    "        - the higher the number and size of hidden layer the smaller the test score, the best the train score and the higher the computation time\n",
    "        - the relu activation function give the best test and train score, followed by the logistic activation function but that one takes more time to compute\n",
    "        - less iterations give the best test score, worst train score and best computation time\n",
    "        - early_stopping set to True is good to optimize the computation time\n",
    "        - higher tolerance gives the quickest computation time but decreases the test and train score\n",
    "        - higher initial learning rate increases the train score but decreases the test score\n",
    "        - validation fraction helps with overfitting so it improves the test score \n",
    "        - learning rate optimize time \n",
    " \n",
    " \n",
    "    - Our best config would be : {\"hidden_layer_sizes\" : (100,25), \"activation\" : 'relu', \"tol\" :0.0001, \"early_stopping\" : True, \"learning_rate\" : 'adaptive',\"validation_fraction\" : 0.1, \"max_iter\":500, \"learning_rate_init\":0.001 }\n",
    "\n",
    "\n",
    "Unless you used cross-validation, you have probably used the \"test\" set to select the best model among the ones you experimented with.\n",
    "Since your model is the one that worked best on the \"test\" set, your selection is *biased*.\n",
    "\n",
    "In all rigor the original dataset should be split in three:\n",
    "\n",
    "- the **training set**, on which each model is trained\n",
    "- the **validation set**, that is used to pick the best parameters of the model \n",
    "- the **test set**, on which we evaluate the final model\n",
    "\n",
    "\n",
    "Evaluate the score of your algorithm on a test set that was not used for training nor for model selection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score 1= -9.00692735001358\n",
      "Test score 2= -0.3160968532235562\n",
      "Test score 3= -0.10173092242689696\n"
     ]
    }
   ],
   "source": [
    "x_test_new = pd.DataFrame(cal_housing.data,columns=cal_housing.feature_names)[2000:2500]\n",
    "Y = cal_housing.target[2000:2500]\n",
    "X_test_new = sc.fit_transform(x_test_new)\n",
    "print(\"Test score 1= \"+ str(final_mlp.score(X_test_new, Y)))\n",
    "\n",
    "x_test_new = pd.DataFrame(cal_housing.data,columns=cal_housing.feature_names)[3000:3500]\n",
    "Y = cal_housing.target[3000:3500]\n",
    "X_test_new = sc.fit_transform(x_test_new)\n",
    "print(\"Test score 2= \"+ str(final_mlp.score(X_test_new, Y)))\n",
    "\n",
    "x_test_new = pd.DataFrame(cal_housing.data,columns=cal_housing.feature_names)[4000:4500]\n",
    "Y = cal_housing.target[4000:4500]\n",
    "X_test_new = sc.fit_transform(x_test_new)\n",
    "print(\"Test score 3= \"+ str(final_mlp.score(X_test_new, Y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with data outside of our training or testing set, the test score are terrible. This is because our algorithm is biased and our training set is small."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
